{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#why-dbt-jobs-as-code","title":"Why <code>dbt-jobs-as-code</code>","text":"<p><code>dbt-jobs-as-code</code> is a tool built to handle dbt Cloud Jobs as a well-defined YAML files. Being standard YAML, it is possible to use YAML anchors to reduce duplicate configuration across jobs.</p> <p>There is also a templating capability to use the same YAML file to update different dbt Cloud projects and/or environments.</p> <p>A given dbt Cloud project can use both <code>dbt-jobs-as-code</code> and jobs degined in the UI at the same time, without any conflict.</p> <p>The way we differentiate jobs defined from code from the ones defined from the UI is that the managed ones have a name ending with <code>[[&lt;identifier&gt;]]</code>.</p> <p>Warning</p> <p>If you plan to use this tool but have existing jobs ending with <code>[[...]]</code> you should rename them before running any command.</p> <p>Below is a video demonstration of how to use dbt-jobs-as-code as part of CI/CD, leveraging the new templating features.</p> <p></p>"},{"location":"#why-not-terraform","title":"Why not Terraform","text":"<p>Terrraform is widely used to manage infrastructure as code. And a comprehensive Terraform provider exists for dbt Cloud, able to manage dbt Cloud jobs (as well as most of the rest of the dbt Cloud configuration like projects, environments, warehouse connections etc...).</p> <p>Terraform is much more powerful but using it requires some knowledge about the tool and requires managing/storing/sharing a state file, containing information about the state of the application.</p> <p>With this package's approach, people don't need to learn another tool and can configure dbt Cloud using YAML, a language used across the dbt ecosystem:</p> <ul> <li>no state file required: the link between the YAML jobs and the dbt Cloud jobs is stored in the jobs name, in the <code>[[&lt;identifier&gt;]]</code> part</li> <li>YAML: dbt users are familiar with YAML and we created a JSON schema allowing people to verify that their YAML files are correct</li> <li>by using filters like <code>--project-id</code>, <code>--environment-id</code> or <code>--limit-projects-envs-to-yml</code> and the templating deature, people can limit the projects and environments checked by the tool, which can be used to \"promote\" jobs between different dbt Cloud environments</li> </ul>"},{"location":"#but-why-not-both","title":"But why not both?","text":"<p>But more than being exclusive from each other, dbt-jobs-as-code and Terraform can be used together:</p> <ul> <li>with dbt-jobs-as-code being used to manage the day to day jobs (handled by the data team) </li> <li>and Terraform being used to manage the rest of the dbt Cloud configuration and even CI jobs (handled by the platform or central team).</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>To see the details of all changes, head to the GitHub repo</p>"},{"location":"changelog/#16","title":"1.6","text":"<ul> <li>Add <code>--filter</code> to <code>import-jobs</code> to allow importing jobs to specific environments. In the case where people maintain jobs in the dbt Cloud UI and want to promote them, they can mention what environments they want to import the jobs to using the identifier of the job: <code>[[envs_filter:identifier]]</code>.</li> </ul>"},{"location":"changelog/#15","title":"1.5","text":"<ul> <li>Add <code>--json</code> to <code>plan</code> and <code>sync</code> to output the <code>stdout</code> changes in JSON format. This can be useful for automating some processes and consuming the changes from scripts. We are still printing logs to <code>stderr</code> though, so to remove those logs you can redirect <code>stderr</code> to <code>/dev/null</code> or redirect <code>stdout</code> to a file and then read from the file.</li> </ul>"},{"location":"changelog/#14","title":"1.4","text":"<ul> <li>Add <code>--templated-fields</code> to <code>import-jobs</code> to add Jinja variables to the generated YAML file. This can be useful to allow users to maintain jobs in the dbt Cloud UI and set a process to automatically promote those to other environments.</li> </ul>"},{"location":"changelog/#13","title":"1.3","text":"<ul> <li>Add this docs site</li> <li>Add <code>--managed-only</code> flag to the <code>import</code> command to only import managed jobs</li> <li>Add <code>--environment-id</code> and <code>--project-id</code> flags to <code>link</code>, <code>unlink</code> and <code>deactivate-jobs</code> commands</li> </ul>"},{"location":"changelog/#12","title":"1.2","text":"<ul> <li>Automatically set the identifier when using <code>import-jobs</code> on managed jobs. This automatically links the jobs to the generated YAML file.</li> </ul>"},{"location":"changelog/#11","title":"1.1","text":"<ul> <li>Add the ability to mention \"glob\" files for the YAML config and var files.<ul> <li>i.e. <code>dbt-jobs-as-code plan \".dbt/jobs/*\"</code> can be used to take into consideration all files in the <code>.dbt/jobs</code> directory.</li> </ul> </li> </ul>"},{"location":"changelog/#10","title":"1.0","text":"<ul> <li>Initial release of <code>1.0</code></li> </ul>"},{"location":"cli/","title":"CLI reference","text":"<p>This page provides documentation for our command line tools.</p>"},{"location":"cli/#dbt-jobs-as-code","title":"dbt-jobs-as-code","text":"<p>dbt-jobs-as-code 1.10.0</p> <p>A CLI to allow defining dbt Cloud jobs as code</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--version</code> boolean Show the version and exit. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#deactivate-jobs","title":"deactivate-jobs","text":"<p>Deactivate jobs triggers in dbt Cloud (schedule and CI/CI triggers) without remoing the jobs.</p> <p>This can be useful when moving jobs from one project to another. When the new jobs have been created, this command can be used to deactivate the jobs from the old project.</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code deactivate-jobs [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--config</code> text The path to your YML jobs config file (or pattern for those files). None <code>--account-id</code> integer The ID of your dbt Cloud account. None <code>--project-id</code>, <code>-p</code> integer [Optional] The ID of dbt Cloud project(s) to use for sync None <code>--environment-id</code>, <code>-e</code> integer [Optional] The ID of dbt Cloud environment(s) to use for sync None <code>--job-id</code>, <code>-j</code> integer The ID of the job to deactivate. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#import-jobs","title":"import-jobs","text":"<p>Generate YML file for import.</p> <p>Either --config or --account-id must be provided to mention what Account ID to use.</p> <p>Optional parameters: --project-id,  --environment-id, --job-id</p> <p>It is possible to repeat the optional parameters --job-id, --project-id, --environment-id option to import specific jobs.</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code import-jobs [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--config</code> text The path to your YML jobs config file (also supports glob patterns for those files or a directory). None <code>--account-id</code> integer The ID of your dbt Cloud account. None <code>--project-id</code>, <code>-p</code> integer [Optional] The ID of dbt Cloud project(s) to use for sync None <code>--environment-id</code>, <code>-e</code> integer [Optional] The ID of dbt Cloud environment(s) to use for sync None <code>--job-id</code>, <code>-j</code> integer [Optional] The ID of the job to import. None <code>--include-linked-id</code> boolean Include the job ID when exporting jobs. <code>False</code> <code>--managed-only</code> boolean Only import jobs that are managed (have an identifier). <code>False</code> <code>--templated-fields</code> text Path to a YAML file containing field templates to apply to the exported jobs. None <code>--filter</code> text Only import jobs where the identifier prefix, before <code>:</code> contains this value, is empty or is '*'. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#link","title":"link","text":"<p>Link the YML file to dbt Cloud by adding the identifier to the job name. All relevant jobs get the part [[...]] added to their name</p> <p>The YAML file will need to contain a <code>linked_id</code> for each job that needs to be linked.</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code link [OPTIONS] CONFIG\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--project-id</code>, <code>-p</code> integer [Optional] The ID of dbt Cloud project(s) to use for sync None <code>--environment-id</code>, <code>-e</code> integer [Optional] The ID of dbt Cloud environment(s) to use for sync None <code>--dry-run</code> boolean In dry run mode we don't update dbt Cloud. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#plan","title":"plan","text":"<p>Check the difference between a local file and dbt Cloud without updating dbt Cloud. This command will not update dbt Cloud.</p> <p>CONFIG is the path to your YML jobs config file (also supports glob patterns for those files or a directory).</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code plan [OPTIONS] CONFIG\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--vars-yml</code>, <code>-v</code> text The path to your vars_yml YML file (or pattern for those files) when using a templated job YML file. None <code>--project-id</code>, <code>-p</code> integer [Optional] The ID of dbt Cloud project(s) to use for sync None <code>--environment-id</code>, <code>-e</code> integer [Optional] The ID of dbt Cloud environment(s) to use for sync None <code>--limit-projects-envs-to-yml</code>, <code>-l</code> boolean [Flag] Limit sync/plan to the projects and environments listed in the jobs YML file <code>False</code> <code>--json</code> boolean Output results in JSON format instead of human-readable text. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#sync","title":"sync","text":"<p>Synchronize a dbt Cloud job config file against dbt Cloud. This command will update dbt Cloud with the changes in the local YML file. It is recommended to run a <code>plan</code> first to see what will be changed.</p> <p>CONFIG is the path to your YML jobs config file (also supports glob patterns for those files or a directory).</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code sync [OPTIONS] CONFIG\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--vars-yml</code>, <code>-v</code> text The path to your vars_yml YML file (or pattern for those files) when using a templated job YML file. None <code>--project-id</code>, <code>-p</code> integer [Optional] The ID of dbt Cloud project(s) to use for sync None <code>--environment-id</code>, <code>-e</code> integer [Optional] The ID of dbt Cloud environment(s) to use for sync None <code>--limit-projects-envs-to-yml</code>, <code>-l</code> boolean [Flag] Limit sync/plan to the projects and environments listed in the jobs YML file <code>False</code> <code>--json</code> boolean Output results in JSON format instead of human-readable text. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#unlink","title":"unlink","text":"<p>Unlink the YML file to dbt Cloud. All relevant jobs get the part [[...]] removed from their name</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code unlink [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--config</code> text The path to your YML jobs config file (or pattern for those files). None <code>--account-id</code> integer The ID of your dbt Cloud account. None <code>--project-id</code>, <code>-p</code> integer [Optional] The ID of dbt Cloud project(s) to use for sync None <code>--environment-id</code>, <code>-e</code> integer [Optional] The ID of dbt Cloud environment(s) to use for sync None <code>--dry-run</code> boolean In dry run mode we don't update dbt Cloud. <code>False</code> <code>--identifier</code>, <code>-i</code> text [Optional] The identifiers we want to unlink. If not provided, all jobs are unlinked. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/#validate","title":"validate","text":"<p>Check that the config file is valid</p> <p>CONFIG is the path to your YML jobs config file (also supports glob patterns for those files or a directory).</p> <p>Usage:</p> <pre><code>dbt-jobs-as-code validate [OPTIONS] CONFIG\n</code></pre> <p>Options:</p> Name Type Description Default <code>--disable-ssl-verification</code> boolean N/A <code>False</code> <code>--vars-yml</code>, <code>-v</code> text The path to your vars_yml YML file (or pattern for those files) when using a templated job YML file. None <code>--online</code> boolean Connect to dbt Cloud to check that IDs are correct. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"getting_started/","title":"Getting Started","text":"<p><code>dbt-jobs-as-code</code> is a CLI built to allow managing dbt Cloud jobs via YAML files, using commands like <code>plan</code>, <code>sync</code> and <code>import-jobs</code>.</p> <p>It calls various endpoints of the dbt Cloud API to create, update and delete jobs.</p>"},{"location":"getting_started/#how-to-install-dbt-jobs-as-code","title":"How to install <code>dbt-jobs-as-code</code>","text":""},{"location":"getting_started/#installing-with-python","title":"Installing with Python","text":"<p>Install from pypi.org (we recommend using a virtual environment):</p> <pre><code>pip install dbt-jobs-as-code # or via any other package manager\n</code></pre> <p>Show the help:</p> <pre><code>dbt-jobs-as-code --help\n</code></pre>"},{"location":"getting_started/#running-as-an-executable-using-uv","title":"Running as an executable using uv","text":"<p>Run <code>dbt-jobs-as-code</code> as a standalone Python executable using <code>uv</code> and <code>uvx</code>:</p> <pre><code>uvx dbt-jobs-as-code --help\n</code></pre>"},{"location":"getting_started/#github-actions","title":"GitHub Actions","text":"<p>Run <code>dbt-jobs-as-code</code> as part of your CI pipeline:</p> <p>See examples in the typical flows page.</p>"},{"location":"getting_started/#pre-requisites","title":"Pre-requisites","text":"<p>The following environment variables are used to run the code:</p> <ul> <li><code>DBT_API_KEY</code>: [Mandatory] The dbt Cloud API key to interact with dbt Cloud. Can be a Service Token (preferred, would require the \"job admin\" scope) or the API token of a given user</li> <li><code>DBT_BASE_URL</code>: [Optional] By default, the tool queries <code>https://cloud.getdbt.com</code>, if your dbt Cloud instance is hosted on another domain, define it in this env variable (e.g. <code>https://emea.dbt.com</code>)</li> </ul>"},{"location":"getting_started/#how-to-use-dbt-jobs-as-code","title":"How to use <code>dbt-jobs-as-code</code>","text":"<p><code>dbt-jobs-as-code</code> can be used in many different ways:</p> <ul> <li>it can be called directly by end users from the CLI</li> <li>it can be triggered by a CI pipeline</li> <li>it can run on a periodic basis to replicate jobs between environments etc...</li> </ul> <p>The list of the different commands and the parameters they accept is available in the CLI documentation.</p>"},{"location":"getting_started/#main-commands","title":"Main commands","text":"<p>The main commands are <code>plan</code> and <code>sync</code>. Both commands require a YML jobs definition file as an input and will replicate the jobs defined in the file to dbt Cloud.</p> Examples of input YML file for <code>plan</code> and <code>sync</code> <p>The YAML file can be created by hand or automatically by using the <code>import-jobs</code> command.</p> jobs.yml<pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/dbt-labs/dbt-jobs-as-code/main/src/dbt_jobs_as_code/schemas/load_job_schema.json\n\njobs:\n  job1:\n    account_id: 43791\n    dbt_version: null\n    deferring_job_definition_id: null\n    environment_id: 134459\n    execute_steps:\n      - \"dbt run --select model1+\"\n      - \"dbt run --select model2+\"\n    execution:\n      timeout_seconds: 0\n    generate_docs: false\n    name: \"My Job 1\"\n    project_id: 176941\n    run_generate_sources: true\n    schedule:\n      cron: \"0 */2 * * *\"\n    settings:\n      target_name: production\n      threads: 4\n    state: 1\n    triggers:\n      git_provider_webhook: false\n      github_webhook: false\n      schedule: true\n\n  job2:\n    account_id: 43791\n    dbt_version: null\n    deferring_job_definition_id: null\n    deferring_environment_id: 43791\n    environment_id: 134459\n    execute_steps:\n      - dbt run-operation clone_all_production_schemas\n      - dbt compile\n    execution:\n      timeout_seconds: 0\n    generate_docs: false\n    name: CI/CD run\n    project_id: 176941\n    run_generate_sources: false\n    schedule:\n      cron: \"0 * * * *\"\n    settings:\n      target_name: TEST\n      threads: 4\n    state: 1\n    triggers:\n      git_provider_webhook: false\n      github_webhook: true # this job runs from webhooks\n      schedule: false # this doesn't run on a schedule\n    custom_environment_variables:\n      - DBT_ENV1: My val\n      - DBT_ENV2: My val2\n</code></pre>"},{"location":"getting_started/#typical-flows","title":"Typical flows","text":"<p>There is also a list of typical flows leveraging <code>dbt-jobs-as-code</code> in the typical flows page.</p> <p>This includes how to leverage the tool to import jobs to different environments, how to set up a CI pipeline to plan and sync jobs, etc...</p>"},{"location":"getting_started/#advanced-features","title":"Advanced features","text":"<p>For more complex use cases, please refer to the advanced features page.</p>"},{"location":"getting_started/#how-to-contribute-to-dbt-jobs-as-code","title":"How to contribute to <code>dbt-jobs-as-code</code>","text":"<p>Today, raising Feature Requests and Issues and providing feedback in the GitHub repository is the best way to help improve this tool.</p>"},{"location":"github_actions_examples/","title":"Github actions examples","text":""},{"location":"github_actions_examples/#todo-add-code","title":"TODO: Add code","text":""},{"location":"typical_flows/","title":"Typical flows","text":"<p>This page descibes how <code>dbt-jobs-as-code</code> can be used in different scenarios, what commands to run in what order, what parameters to use, what CI actions to create etc...</p>"},{"location":"typical_flows/#cicd-flows-for-most-use-cases","title":"CI/CD flows for most use cases","text":""},{"location":"typical_flows/#manage-jobs-as-code-only-for-production","title":"Manage jobs as code only for Production","text":"<p>In the most basic case, people maintain their jobs YAML files in their git repository and configure some CI action to trigger: </p> <ul> <li>a <code>plan</code> on PR creation to get a feedback of the planned changes</li> <li>a <code>sync</code> on PR approval to apply the changes to production</li> </ul> <pre><code>sequenceDiagram\n    actor U as User\n    participant C as dbt Cloud\n    participant G as git provider\n    actor U2 as Other User\n\n    U -&gt;&gt; G: creates a PR with dbt code and dbt-jobs-as-code YML\n    G --&gt;&gt; C: run `plan` on PR creation\n    C --&gt;&gt; G: get the planned changes\n    U2 -&gt;&gt; G: approve PR\n    G --&gt;&gt; C: run `sync` on PR approval\n    C --&gt;&gt; G: get confirmation that the jobs were updated</code></pre> <p>The relevant GitHub actions would be the following:</p> Examples of Github actions for <code>plan</code> and <code>sync</code> plan_on_pr.yml<pre><code>name: Plan for syncing dbt Cloud Jobs from YAML\nrun-name: Running dbt-jobs-as-code plan to see what is going to be changed/deleted\n\non:\n  pull_request:\n    branches:\n      - main\n    paths:\n      - 'jobs/**'\n\njobs:\n  run-dbt-jobs-as-code:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.12.x\"\n\n        - name: Install a specific version of uv\n          uses: astral-sh/setup-uv@v5\n          with:\n            version: \"0.5\"\n\n      - name: Run dbt-jobs-as-code\n        run: uvx dbt-jobs-as-code plan jobs/my_jobs.yml # (1)!\n        # or to limit the code to the envs/projects in the YML file\n        # run: uvx dbt-jobs-as-code plan jobs/my_jobs.yml --limit-projects-envs-to-yml\n        env: \n          DBT_API_KEY: \"${{secrets.DBT_API_KEY}}\"\n          # DBT_BASE_URL is optional (2)\n</code></pre> <ol> <li>We could also force a specific version with <code>uvx dbt-jobs-as-code@1.2</code></li> <li>It is set by default to <code>https://cloud.getdbt.com</code>. We could add <code>DBT_BASE_URL: \"${{secrets.DBT_BASE_URL}}\"</code> if we wanted to use a secret for our base url</li> </ol> sync_on_pr_closed.yml<pre><code>name: Replicate dbt Cloud Jobs from YAML\nrun-name: Running dbt-jobs-as-code sync to replicate dbt Cloud jobs from YAML\n\non:\n  pull_request:\n    types:\n      - closed\n    branches:\n      - main\n    paths:\n      - 'jobs/**'\n\njobs:\n  run-dbt-jobs-as-code:\n    if: github.event.pull_request.merged == true\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.12.x\"\n\n        - name: Install a specific version of uv\n          uses: astral-sh/setup-uv@v5\n          with:\n            version: \"0.5\"\n\n      - name: Run dbt-jobs-as-code\n        run: uvx dbt-jobs-as-code sync jobs/my_jobs.yml # (1)!\n        # or to limit the code to the envs/projects in the YML file\n        # run: uvx dbt-jobs-as-code sync jobs/my_jobs.yml --limit-projects-envs-to-yml\n        env: \n          DBT_API_KEY: \"${{secrets.DBT_API_KEY}}\"\n</code></pre> <ol> <li>We could also force a specific version with <code>uvx dbt-jobs-as-code@1.2</code></li> </ol> <p>The relevant GitLab pipelines would be the following:</p> Examples of GitLab pipelines for <code>plan</code> and <code>sync</code> .gitlab-ci.yml<pre><code>stages:\n  - plan\n  - sync\n\n\n# This job only runs on merge requests and when files in jobs/ change\nplan_dbt_jobs:\n  stage: plan\n  image: python:3.12 \n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\" &amp;&amp; $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == \"main\"\n      changes:\n        - jobs/**/*\n  script:\n    - apt-get update &amp;&amp; apt-get install -y jq git\n    - pip install dbt-jobs-as-code\n    - |\n      echo \"Checking out branch...\"\n      git fetch origin $CI_MERGE_REQUEST_TARGET_BRANCH_NAME\n      git fetch origin $CI_COMMIT_REF_NAME\n      git checkout $CI_COMMIT_REF_NAME \n    - dbt-jobs-as-code plan jobs/jobs_config.yml\n\n  before_script:\n    - export DBT_API_KEY=$DBT_API_KEY  # Ensure this variable is set in GitLab CI/CD project settings\n\n\nsync_dbt_jobs:\n  stage: sync\n  image: python:3.12 \n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\" &amp;&amp; $CI_COMMIT_BRANCH == \"main\" &amp;&amp; $CI_COMMIT_MESSAGE =~ /^Merge.*/\n      changes:\n        - jobs/**/*\n\n  script:\n    - apt-get update &amp;&amp; apt-get install -y jq git\n    - pip install dbt-jobs-as-code\n    - |\n      echo \"Checking out branch...\"\n      git fetch origin $CI_COMMIT_BRANCH      \n      git checkout $CI_COMMIT_BRANCH    \n    - dbt-jobs-as-code sync jobs/jobs_config.yml \n\n  before_script:\n    - export DBT_API_KEY=$DBT_API_KEY  # Ensure this variable is set in GitLab CI/CD project settings\n</code></pre>"},{"location":"typical_flows/#manage-jobs-as-code-in-different-environments","title":"Manage jobs as code in different environments","text":"<p>A similar approach to the first case, but in that situation, we use templated YML files so that the same YML file can be used across different environments, and jobs can be configured based on some variables.</p> <p>In that case, both promotion to QA and promotion to Prod will trigger a <code>plan</code> on PR creation and a <code>sync</code> on PR approval.</p> <pre><code>sequenceDiagram\n    actor U as User\n    participant C as dbt Cloud\n    participant G as git provider\n    actor U2 as Other User\n\n    U -&gt;&gt; G: creates a PR to `qa` with dbt and dbt-jobs-as-code code\n    Note over U,G: the YAML will use templated variables &lt;br&gt;for the differences between qa and prod\n\n    G --&gt;&gt; C: run `plan` on PR creation using QA \n    C --&gt;&gt; G: get the planned changes for the QA env\n    U2 -&gt;&gt; G: approve PR\n    G --&gt;&gt; C: run `sync` on PR approval using QA variables\n    C --&gt;&gt; G: get confirmation that the jobs were updated\n\n    U2 -&gt;&gt; G: creates a PR from `qa` to `prod`, from the git interface\n    G --&gt;&gt; C: run `plan` on PR creation using prod variables\n    C --&gt;&gt; G: get the planned changes for the prod env\n    U2 -&gt;&gt; G: approve PR\n    G --&gt;&gt; C: run `sync` on PR approval using prod variables\n    C --&gt;&gt; G: get confirmation that the jobs were updated\n</code></pre> <p>The GitHub action is now a bit more complex to manage any number of promotion environments.</p> <p>By updating <code>BRANCH_FILE_MAP</code>, we can map different branches of our different environments to different variable files.</p> <p>The example below is just for <code>plan</code> on PR creation, but the same logic would apply for <code>sync</code> on PR approval.</p> Examples of Github actions for <code>plan</code> plan_on_pr.yml<pre><code>name: Plan for syncing dbt Cloud Jobs from YAML\nrun-name: Running dbt-jobs-as-code plan to see what is going to be changed/deleted\n\non:\n  pull_request:\n    branches:\n      - '**'  # Trigger on all branches\n    paths:\n      - 'jobs/**' # we only trigger when files change under the `jobs/` folder which is where we store our config\n\nenv:\n  BRANCH_FILE_MAP: |\n    {\n      \"main\": \"jobs/vars_prod.yml\",\n      \"qa\": \"jobs/vars_qa.yml\"\n    }\n\njobs:\n  run-dbt-jobs-as-code:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.12.x\"\n\n      - name: Install a specific version of uv\n          uses: astral-sh/setup-uv@v5\n          with:\n            version: \"0.5\"\n\n      - name: Set vars file based on branch\n        id: set-vars-file\n        run: |\n          target_branch=\"${{ github.base_ref }}\"\n          vars_file=$(echo '${{ env.BRANCH_FILE_MAP }}' | jq -r --arg branch \"$target_branch\" '.[$branch] // empty')\n\n          if [[ -z \"$vars_file\" ]]; then\n            echo \"Branch $branch not found in BRANCH_FILE_MAP. Exiting.\"\n            exit 0  # Exit gracefully if branch is not in the map\n          fi\n\n          echo \"vars_file=$vars_file\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Run dbt-jobs-as-code\n        if: steps.set-vars-file.outputs.vars_file != ''  # Only run if vars_file is set\n        run: uvx dbt-jobs-as-code plan jobs/jobs.yml --vars-yml ${{ steps.set-vars-file.outputs.vars_file }} --limit-projects-envs-to-yml\n        env: \n          DBT_API_KEY: \"${{secrets.DBT_API_KEY}}\"\n</code></pre> <p>The relevant GitLab pipelines would be the following:</p> Examples of GitLab pipelines for <code>plan</code> and <code>sync</code> gitlab-ci.yml<pre><code>stages:\n  - plan\n  - sync\n\nvariables:\n  BRANCH_FILE_MAP: |\n    {\n      \"main\": \"jobs/vars_prod.yml\",\n      \"qa\": \"jobs/vars_qa.yml\"\n    }\n\n# This job only runs on merge requests and when files in jobs/ change\nplan_dbt_jobs:\n  stage: plan\n  image: python:3.12 \n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      changes:\n        - jobs/**/*\n  script:\n    - apt-get update &amp;&amp; apt-get install -y jq git\n    - pip install dbt-jobs-as-code\n    - |\n      echo \"Checking out branch...\"\n      git fetch origin $CI_MERGE_REQUEST_TARGET_BRANCH_NAME\n      git fetch origin $CI_COMMIT_REF_NAME\n      git checkout $CI_COMMIT_REF_NAME\n    - |\n      echo \"Determining vars file for target branch: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME\"\n      VARS_FILE=$(echo \"$BRANCH_FILE_MAP\" | jq -r --arg branch \"$CI_MERGE_REQUEST_TARGET_BRANCH_NAME\" '.[$branch] // empty')\n      if [ -z \"$VARS_FILE\" ]; then\n        echo \"Branch $CI_MERGE_REQUEST_TARGET_BRANCH_NAME not found in BRANCH_FILE_MAP. Skipping job.\"\n        exit 0\n      fi\n      echo \"Using vars file: $VARS_FILE\"\n    - dbt-jobs-as-code plan jobs/my_jobs.yml --vars-yml \"$VARS_FILE\" --limit-projects-envs-to-yml\n  before_script:\n    - export DBT_API_KEY=$DBT_API_KEY  # Ensure this variable is set in GitLab CI/CD project settings\n\n\nsync_dbt_jobs:\n  stage: sync\n  image: python:3.12 \n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\" &amp;&amp; $CI_COMMIT_MESSAGE =~ /^Merge.*/\n      changes:\n        - jobs/**/*\n\n  script:\n    - apt-get update &amp;&amp; apt-get install -y jq git\n    - pip install dbt-jobs-as-code\n    - |\n      echo \"Checking out branch...\"\n      git fetch origin $CI_COMMIT_BRANCH      \n      git checkout $CI_COMMIT_BRANCH\n    - |\n      echo \"Determining vars file for target branch: $CI_COMMIT_BRANCH\"\n      VARS_FILE=$(echo \"$BRANCH_FILE_MAP\" | jq -r --arg branch \"$CI_COMMIT_BRANCH\" '.[$branch] // empty')\n      if [ -z \"$VARS_FILE\" ]; then\n        echo \"Branch $CI_COMMIT_BRANCH not found in BRANCH_FILE_MAP. Skipping job.\"\n        exit 0\n      fi\n      echo \"Using vars file: $VARS_FILE\"\n    - dbt-jobs-as-code sync jobs/my_jobs.yml --vars-yml \"$VARS_FILE\" --limit-projects-envs-to-yml\n  before_script:\n    - export DBT_API_KEY=$DBT_API_KEY  # Ensure this variable is set in GitLab CI/CD project settings\n</code></pre>"},{"location":"typical_flows/#flow-for-the-initial-import-of-jobs","title":"Flow for the initial import of jobs","text":""},{"location":"typical_flows/#initial-import-of-jobs","title":"Initial import of jobs","text":"<p>The <code>import-jobs</code> command can be used to import jobs from dbt Cloud to a YAML file. Once the YAML file has been created it can be pushed to a git repository and jobs can be managed as code as in the previous cases.</p> <p>The developer would just run <code>dbt-jobs-as-code import-jobs</code> and decide the parameters for the import:</p> <ul> <li><code>--project-id</code>: the project to import the jobs from</li> <li><code>--environment-id</code>: the environment to import the jobs from</li> <li><code>--include-linked-id</code>: whether to include the linked id in the jobs</li> <li><code>--only-managed</code>: whether to only import \"managed\" jobs and leave the other one to be still managed via the UI in dbt Cloud</li> </ul> <p>Once the YAML file has been created, it can be modified to remove jobs or change some parameters to replace them by variables.</p> <pre><code>sequenceDiagram\n    actor U as User\n    participant C as dbt Cloud\n    participant G as git provider\n\n    U -&gt;&gt; C: run `import-jobs`\n    Note over U,C: Can specify the project, env and &lt;br&gt;whether to only import &lt;br&gt;\"managed\" jobs\n    C -&gt;&gt; U: get yaml file\n    Note over U: Update YAML if needed\n    U -&gt;&gt; G: push YAML file to repo</code></pre>"},{"location":"typical_flows/#flows-for-once-off-migrations-of-jobs","title":"Flows for once off migrations of jobs","text":""},{"location":"typical_flows/#replicate-jobs-from-one-environment-to-another-once-off","title":"Replicate jobs from one environment to another (once off)","text":"<p>In the case that the requirement is only to replicate some or all jobs as a once off, a developer could run a few different commands without pushing the YAML file to a git repository.</p> <pre><code>sequenceDiagram\n    actor U as User\n    participant C as dbt Cloud\n\n    U -&gt;&gt; C: run `import-jobs`\n    Note over U,C: Can specify the project, env and &lt;br&gt;whether to only import &lt;br&gt;\"managed\" jobs\n    C -&gt;&gt; U: get yaml file\n    Note over U: Update YAML if needed\n    U -&gt;&gt; C: run `plan` and `sync` to push jobs\n    U -&gt;&gt; C: run `unlink` to remove the identifier from the jobs\n</code></pre>"},{"location":"typical_flows/#move-all-jobs-from-one-environment-to-another","title":"Move all jobs from one environment to another","text":"<p>This flow can be useful for people who want to move all jobs from one environment/project to another. This can happen for example when going from a monorepo to a dbt-mesh and wanting to move some/all the jobs to the new dbt Cloud project.</p> <p>The initial steps are the same as for replicating jobs as a once off but we run a few extra commands to deactivate the jobs in the original project and envs.</p> <pre><code>sequenceDiagram\n    actor U as User\n    participant C as dbt Cloud\n\n    U -&gt;&gt; C: run `import-jobs`\n    Note over U,C: Can specify the project, env and &lt;br&gt;whether to only import &lt;br&gt;\"managed\" jobs\n    C -&gt;&gt; U: get yaml file\n    Note over U: Update YAML if needed to update/remove jobs&lt;br&gt;and update project and env ids\n    U -&gt;&gt; C: run `link` to manage jobs in the monorepo environment\n    U -&gt;&gt; C: run `deactivate-jobs` on the original project and envs\n    U -&gt;&gt; C: [optional] run `unlink` in the required envs to remove the identifier from the jobs</code></pre>"},{"location":"typical_flows/#advanced-flows","title":"Advanced flows","text":""},{"location":"typical_flows/#automated-promotion-of-jobs-created-in-the-ui-to-other-environments","title":"Automated promotion of jobs created in the UI to other environments","text":"<p>In this mechanism, we use <code>dbt-jobs-as-code</code> to create a CI/CD pipeline to automatically promote jobs created in the dbt Cloud UI to other environments. It would synchronize jobs and allow creating new ones, deleting old ones and modifying existing ones.</p> <p>The key part of the process is the advanced ability to import jobs while setting Jinja variables that will be rendered differently for each environment.</p> <pre><code>sequenceDiagram\n    actor D as dbt Developer\n    participant C as dbt Cloud\n    participant G as git repo\n    actor U as User\n\n    D --&gt;&gt;C: Maintain dbt Cloud jobs in the UI in dev,&lt;br&gt; adding [[..]] to the jobs that&lt;br&gt; need to be promoted/replicated\n    U --&gt;&gt;G: Create and commit the YAML vars files for each env\n    U --&gt;&gt;G: Create and commit the YAML vars files for the templated values\n    U --&gt;&gt;G: Create a pipeline to automatically run `dbt-jbos-as-code&lt;br&gt;on a schedule or triggered by somone\n    G -&gt;&gt; C: run `import-jobs` with `--templated-fields templ.yml`&lt;br&gt;,`--managed-only` and `-e` to specify the env\n    C -&gt;&gt; G: get yaml file\n    Note over G: Check difference with existing file&lt;br&gt; and auto-create PR if it is different\n    Note right of G: When the PR is created, we follow the&lt;br&gt; typical flow of GH actions&lt;br&gt; with plan/sync on PR creation/merge\n</code></pre>"},{"location":"advanced_config/","title":"Advanced configuration","text":"<p>More advanced features of <code>dbt-jobs-as-code</code> can be combined to match complex requirements in regards to dbt Cloud jobs management.</p> <ul> <li>YAML Templating - for using the same YAML file to update different dbt Cloud projects and/or environments</li> <li>glob config files - for using glob patterns to match config files at once</li> <li>YAML anchors - to reuse the same parameters in different jobs</li> <li>Advanced jobs importing - for importing jobs from dbt Cloud to a YAML file</li> </ul>"},{"location":"advanced_config/glob_config_files/","title":"glob config files","text":"<p>The different commands that require a config file and/or a variables file as parameters (see command details here) can use glob patterns instead of just file names to match multiple files.</p> <p>Those patterns are also called \"Unix style pathname pattern expansion\", and in a nutshell:</p> <ul> <li><code>*</code> matches any sequence of characters, in a directory or a file name</li> <li><code>**</code> matches any sequence of characters, including multiple directories</li> <li><code>?</code> matches any single character</li> </ul> <p>For example, to run the <code>plan</code> command on all the files stored in subdirectoris under the <code>jobs</code> directory, you can use the following command:</p> <pre><code>dbt-jobs-as-code plan \"jobs/**/*.yml\" # (1)!\n</code></pre> <ol> <li>Depending on your shell you might have to quote the pattern or not. For example, for <code>zsh</code> quoting is required as otherwise the shell will try to expand the pattern before passing it to the command.</li> </ol> <p>If the provided config is a directory, we automatically search for all the <code>*.yml</code> files in this directory. This is particularly relevant for users with a shell not supporting the <code>*</code> character.</p>"},{"location":"advanced_config/jobs_importing/","title":"Advanced job importing","text":""},{"location":"advanced_config/jobs_importing/#generating-jobs-with-templated-fields-with-import-jobs","title":"Generating jobs with templated fields with <code>import-jobs</code>","text":"<p><code>plan</code> and <code>sync</code> commands allow adding Jinja variables to the jobs in order to use the same YAML file for different environments (see YAML templating).</p> <p>While it is possible to import the jobs from dbt Cloud using the <code>import-jobs</code> command and to modify the outputted YAML by hand to add variables, there is also the ability to use a YAML file to specify variables for the different jobs.</p> <p>By providing the <code>--templated-fields</code> parameter, it is possible to use a YAML file to specify the variables for specific fields of the jobs.</p> <p>For example, the following YAML file:</p> templ.yml<pre><code>environment_id: {{ environment_id }}\ndeferring_environment_id: {{ deferring_environment_id }}\ntriggers.schedule: {{ env_name == 'prod' }}\n</code></pre> <p>will be used to set the <code>project_id</code>, <code>environment_id</code>, <code>deferring_environment_id</code> and <code>triggers.schedule</code> fields for all the jobs in the generated YAML file and can be called with </p> <pre><code>dbt-jobs-as-code import-jobs --account-id 1234 --project-id 3213 --environment-id 423432 --templated-fields templ.yml --managed-only\n</code></pre> <p>The outputted YAML will look like the following:</p> jobs.yml<pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/dbt-labs/dbt-jobs-as-code/main/src/dbt_jobs_as_code/schemas/load_job_schema.json\n\njobs:\n  my-id:\n    account_id: 1234\n    project_id: 3213\n    environment_id: {{ environment_id }}\n    dbt_version:\n    name: My job name created from the UI\n    settings:\n      threads: 10\n      target_name: default\n    execution:\n      timeout_seconds: 100\n    deferring_job_definition_id:\n    deferring_environment_id: {{ deferring_environment_id }}\n    run_generate_sources: true\n    execute_steps:\n      - dbt build\n    generate_docs: true\n    schedule:\n      cron: 0 1,5 * * 0,1,2,3,4,5\n    triggers:\n      github_webhook: false\n      git_provider_webhook: false\n      schedule: {{ env_name == 'prod' }}\n      on_merge: false\n    description: ''\n    run_compare_changes: false\n    compare_changes_flags: --select state:modified\n    job_type: other\n    triggers_on_draft_pr: false\n    job_completion_trigger_condition:\n    custom_environment_variables: []\n</code></pre>"},{"location":"advanced_config/jobs_importing/#automatically-promote-jobs-between-environments","title":"Automatically promote jobs between environments","text":""},{"location":"advanced_config/jobs_importing/#generating-jobs-yml-file-as-part-of-a-cicd-process","title":"Generating jobs YML file as part of a CI/CD process","text":"<p>The import command from above can also be used to automatically promote jobs between environments. In that case, as part of a CI/CD process, or on a schedule, the following command can be used to generate the YAML content and save it to a file:</p> <pre><code>dbt-jobs-as-code import-jobs --account-id 1234 --project-id 3213 --environment-id 423432 --templated-fields templ.yml --managed-only &gt; jobs.yml\n</code></pre> <p>It would then be possible to automate the creation of PRs whenever the <code>jobs.yml</code> file is updated, meaning that some jobs would have been updated in the dbt Cloud UI. The GitHub action Create Pull Request could be used to implement this flow.</p> <p>Then, the <code>jobs.yml</code> file can be used to import the jobs in a different environment with the following command, like described in YAML templating and in the typical flows page :</p> <pre><code>dbt-jobs-as-code plan jobs.yml -v qa_vars.yml --limit-projects-envs-to-yml\n</code></pre> <p>With <code>qa_vars.yml</code> being the YAML file containing the variables for the QA/staging environment.</p> qa_vars.yml<pre><code>env_name: \"qa\"\nenvironment_id: 456\ndeferring_environment_id: # (1)!\n</code></pre> <ol> <li>The <code>deferring_environment_id</code> here is set to the null value, we could also set it to a specific ID</li> </ol> <p>And <code>prod_vars.yml</code> being the YAML file containing the variables for the Prod environment.</p> prod_vars.yml<pre><code>env_name: \"prod\"\nenvironment_id: 789\ndeferring_environment_id: \n</code></pre>"},{"location":"advanced_config/jobs_importing/#defining-some-jobs-to-be-imported-to-only-specific-environments","title":"Defining some jobs to be imported to only specific environments","text":"<p>It is possible to define some jobs to be imported to only specific environments by using the <code>--filter</code> parameter for <code>dbt-jobs-as-code import-jobs</code> and by using the correct identifier for the job.</p> <p>To do, so the identifier of the job should be in the format <code>[[envs_filter:identifier]]</code>. The rules are the following:</p> <ul> <li>when <code>--filter</code> is not provided, all jobs are imported</li> <li>when <code>--filter</code> is provided, the following jobs are imported<ul> <li>the jobs with an <code>envs_filter</code> that contains the filter are imported</li> <li>the jobs with an <code>envs_filter</code> that is empty are imported</li> <li>the jobs with an <code>envs_filter</code> equal to <code>*</code> are imported</li> </ul> </li> </ul> <p>As an example, if a job is named <code>My daily job [[uat:my-daily-job]]</code> :</p> <ul> <li><code>dbt-jobs-as-code import-jobs ... --filter uat</code> will import the job \u2705</li> <li><code>dbt-jobs-as-code import-jobs ...</code> without a filter will import the job \u2705</li> <li><code>dbt-jobs-as-code import-jobs ... --filter prod</code> will not import the job \u274c</li> </ul> <p>This feature allows developers to define up to which environment jobs should be imported. If a job needs to be tested in UAT before moving to Prod, the developper can add the filter <code>uat</code> to the identifier of the job and the automated import script would use the <code>--filter uat</code> parameter.</p> <p>The automated promotion process would then create different jobs YML file for each environment.</p> <p>Finally, when the job can be moved to Prod, the developer can remove the filter from the identifier of the job (or replace it with <code>*</code>) and the automated import script will not use the <code>--filter</code> parameter.</p> <p>When to use this feature</p> <p>This feature should be used only when wanting to let people create jobs in the dbt Cloud UI in an environment and get those promoted to higher environments.</p> <p>When possible, it is advised to maintian the jobs YAML file manually and to include both the dbt code and the job definition in the same PR.</p>"},{"location":"advanced_config/templating/","title":"YAML Templating","text":""},{"location":"advanced_config/templating/#templating-jobs-yaml-file","title":"Templating jobs YAML file","text":"<p><code>validate</code>, <code>sync</code> and <code>plan</code> support templated YML jobs file since version 0.6.0</p> <p>To add templated values to your jobs YAML file:</p> <ul> <li>update the jobs YAML file by setting some values as Jinja variables<ul> <li>e.g <code>project_id: {{ project_id }}</code> or <code>environment_id: {{ environment_id }}</code></li> </ul> </li> <li>and add the parameter <code>--vars-yml</code> (or <code>-v</code>) pointing to a YAML file containing values for your variables</li> </ul> <p>The file called in <code>--vars-yml</code> needs to be a valid YAML file like the following:</p> vars_qa.yml<pre><code>project_id: 123\nenvironment_id: 456\n</code></pre> <p>Templating also allows people to version control those YAML files and to have different files for different development layers, like:</p> <ul> <li><code>dbt-jobs-as-code jobs.yml --vars-yml vars_qa.yml --limit-projects-envs-to-yml</code> for QA</li> <li><code>dbt-jobs-as-code jobs.yml --vars-yml vars_prod.yml --limit-projects-envs-to-yml</code> for Prod</li> </ul> Example of templated jobs YAML file and variables files: jobs.yml<pre><code>jobs:\n\n    job1:\n        account_id: 43791\n        project_id: \"{{project_id}}\"\n        environment_id: \"{{ environment_id }}\"\n        dbt_version:\n        name: My Job 1 with a new name\n        settings:\n        threads: 4\n        # we can use the typical Jinja concatenation\n        target_name: \"{{ env_type ~ 'uction' }}\"\n        execution:\n        timeout_seconds: 0\n        deferring_environment_id:\n        run_generate_sources: true\n        execute_steps:\n            - dbt run --select model1+\n            - dbt run --select model2+\n            - dbt compile\n        generate_docs: false\n        run_compare_changes: false\n        schedule:\n        cron: 0 */2 * * *\n        triggers:\n        github_webhook: false\n        git_provider_webhook: false\n        # we can put some logic to decide for true/false\n        schedule: \"{{ env_type == 'prod' }}\"\n        on_merge: false\n        job_completion_trigger_condition:\n\n\n    job2:\n        account_id: 43791\n        project_id: \"{{project_id}}\"\n        environment_id: \"{{environment_id}}\"\n        dbt_version:\n        name: CI/CD run\n        settings:\n        threads: 4\n        target_name: TEST\n        deferring_environment_id:\n        run_generate_sources: true\n        run_compare_changes: true\n        execute_steps:\n            - dbt run-operation clone_all_production_schemas\n            - dbt compile\n        generate_docs: true\n        schedule:\n        cron: 0 * * * *\n        triggers:\n        github_webhook: true\n        git_provider_webhook: false\n        schedule: false\n        on_merge: true\n        job_type: other\n        triggers_on_draft_pr: true\n        job_completion_trigger_condition:\n        condition:\n            job_id: 123\n            project_id: 234\n            statuses:\n                - 10\n                - 20\n        custom_environment_variables:\n            - DBT_ENV1: My val\n            - DBT_ENV2: My val2\n</code></pre> <p>With the following corresponding variables files:</p> vars_qa.yml<pre><code>env_type: qa\nproject_id: 123\nenvironment_id: 456\ndeferring_environment_id: 789\n</code></pre> vars_prod.yml<pre><code>env_type: prod\nproject_id: 12\nenvironment_id: 231231\ndeferring_environment_id: 33213\n</code></pre> <p>The example above is also available under <code>example_jobs_file/jobs_templated...</code> in the repo.</p>"},{"location":"advanced_config/templating/#additional-considerations","title":"Additional considerations","text":"<p>When using templates, you might also want to use the flag <code>--limit-projects-envs-to-yml</code>. This flag will make sure that only the projects and environments of the rendered YAML files will be checked to see what jobs to create/delete/update.</p> <p>The tool will raise errors if:</p> <ul> <li>the jobs YAML file provided contains Jinja variables but <code>--vars-yml</code> is not provided</li> <li>the jobs YAML file provided contains Jinja variables that are not listed in the <code>--vars-yml</code> file</li> </ul>"},{"location":"advanced_config/yaml_anchors/","title":"Using YAML anchors","text":"<p>As jobs are defined in YAML files, we can use YAML anchors to avoid duplicating the same values in multiple jobs.</p> <p>For example, we can define a common <code>settings</code> section in an anchor and then reuse it in all our jobs:</p> jobs.yml<pre><code>anchors:\n    schedule: &amp;every_hour # using a non-job as an anchor\n    cron: \"0 * * * *\"\n    date:\n        cron: \"0 * * * *\"\n        type: \"custom_cron\"\n    time:\n        hours: null\n        interval: 1\n        type: every_hour\n    ids_prod_env: &amp;ids_prod_env\n    account_id: 43791\n    environment_id: 134459\n    project_id: 176941\n\njobs:\n    job1: &amp;main_job # using parameters of a job as the anchor\n    &lt;&lt;: *ids_prod_env\n    dbt_version: null\n    deactivated: false\n    deferring_job_definition_id: null\n    deferring_environment_id: null\n    execute_steps:\n    - \"dbt run --select model1+\"\n    - \"dbt run --select model2+\"\n    - \"dbt compile\"\n    execution:\n        timeout_seconds: 0\n    generate_docs: false\n    generate_sources: true\n    name: \"My Job 1 with a new name\"\n    run_generate_sources: true\n    schedule:\n        cron: \"0 */2 * * *\"\n        date:\n        cron: \"0 */2 * * *\"\n        type: \"custom_cron\"\n        time:\n        type: \"every_hour\"\n        interval: 1\n    settings:\n        target_name: production\n        threads: 4\n    state: 1\n    triggers:\n        git_provider_webhook: false\n        github_webhook: false\n        schedule: true\n    custom_environment_variables: []\n    job2:\n    &lt;&lt;: *main_job # &lt;&lt; means that we take all the values from the first job but we then overwrite them\n    deferring_job_definition_id: null\n    deferring_environment_id: null\n    execute_steps:\n    - dbt run-operation clone_all_production_schemas\n    - dbt compile\n    generate_docs: true\n    generate_sources: true # what does it do??\n    name: CI/CD run\n    run_generate_sources: true\n    schedule: *every_hour # * means that we take the value as-is\n    settings:\n        target_name: TEST\n        threads: 4\n    triggers:\n        git_provider_webhook: false\n        github_webhook: true # this job runs from webhooks\n        schedule: false # this doesn't run on a schedule\n    custom_environment_variables:\n        - DBT_ENV1: My val\n        - DBT_ENV2: My val2\n</code></pre> <p>We can use anchors when using glob patterns for the config and variables files, but the anchors won't be \"shared\" across files.</p>"}]}